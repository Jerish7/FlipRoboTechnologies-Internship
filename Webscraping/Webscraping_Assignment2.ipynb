{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. First, get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(r\"D://chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the required elements from the search bars of job and location\n",
    "job_search=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "location_search=driver.find_element_by_id('qsb-location-sugg')\n",
    "\n",
    "#Sending the inputs to the webpage\n",
    "job_search.send_keys(\"Data Analyst\")\n",
    "location_search.send_keys(\"Bangalore\")\n",
    "\n",
    "#Searching the inputs by using the search button and clicking it\n",
    "driver.find_element_by_xpath(\"//div[@class='search-btn']/button\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the url of the webpage to be scraped\n",
    "url=\"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company name</th>\n",
       "      <th>Experience required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Engineer/Data Analyst- Chennai</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Bangalo...</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Capgemini Technology Services India Limited</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>RedLock, Inc</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru(Devalapur)</td>\n",
       "      <td>Super India Tech Mark</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>CONDUENT BUSINESS SERVICES INDIA LLP</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst - Alteryx, Tableau and SQL</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Schneider Electric</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Title  \\\n",
       "0      Data Engineer/Data Analyst- Chennai   \n",
       "1                       Azure Data Analyst   \n",
       "2                             Data Analyst   \n",
       "3                             Data Analyst   \n",
       "4                             Data Analyst   \n",
       "5  Data Analyst - Alteryx, Tableau and SQL   \n",
       "6                             Data Analyst   \n",
       "7                             Data Analyst   \n",
       "8                             Data Analyst   \n",
       "9                             Data Analyst   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Hyderabad/Secunderabad, Pune, Chennai, Bangalo...   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                     Bangalore/Bengaluru(Devalapur)   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                  Company name Experience required  \n",
       "0           Inflexion Analytix Private Limited             0-2 Yrs  \n",
       "1  Capgemini Technology Services India Limited             6-8 Yrs  \n",
       "2                                 RedLock, Inc             0-0 Yrs  \n",
       "3                        Super India Tech Mark             0-2 Yrs  \n",
       "4         CONDUENT BUSINESS SERVICES INDIA LLP             1-2 Yrs  \n",
       "5                           Schneider Electric             2-5 Yrs  \n",
       "6                     Myntra Designs Pvt. Ltd.             3-6 Yrs  \n",
       "7                     Myntra Designs Pvt. Ltd.             4-8 Yrs  \n",
       "8                     Myntra Designs Pvt. Ltd.             3-8 Yrs  \n",
       "9                     Myntra Designs Pvt. Ltd.             4-8 Yrs  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting all the tags having the job title\n",
    "job_title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "job_title\n",
    "\n",
    "#Extracting the text from the tags\n",
    "title=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in job_title[:10]:\n",
    "    title.append(i.text)\n",
    "title\n",
    "\n",
    "#Extracting all the tags having the job location\n",
    "job_location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "job_location\n",
    "\n",
    "#Extracting the text from the tags\n",
    "location=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in job_location[:10]:\n",
    "    location.append(i.text)\n",
    "location    \n",
    "\n",
    "#Extracting the tags having the company name\n",
    "name=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "name\n",
    "\n",
    "#Extracting the text from the tags\n",
    "company=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in name[:10]:\n",
    "    company.append(i.text)\n",
    "company\n",
    "\n",
    "#Extracting the tags having experience required\n",
    "exp_required=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\")\n",
    "exp_required\n",
    "\n",
    "#Extracting the text from the tags\n",
    "exp=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in exp_required[:10]:\n",
    "    exp.append(i.text)\n",
    "exp\n",
    "\n",
    "#Checking out the length of the data extracted\n",
    "print(len(title),len(location),len(company),len(exp))\n",
    "\n",
    "#Creating a new dataframe for saving the data\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['Title']=title\n",
    "jobs['Location']=location\n",
    "jobs['Company name']=company\n",
    "jobs['Experience required']=exp\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. First, get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(r\"D://chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the required elements from the search bars of job and location\n",
    "job_search=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "location_search=driver.find_element_by_id('qsb-location-sugg')\n",
    "\n",
    "#Sending the inputs to the webpage\n",
    "job_search.send_keys(\"Data Scientist\")\n",
    "location_search.send_keys(\"Bangalore\")\n",
    "\n",
    "#Searching the inputs by using the search button and clicking it\n",
    "driver.find_element_by_xpath(\"//div[@class='search-btn']/button\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the url of the webpage to be scraped\n",
    "url=\"https://www.naukri.com/data-scientist-jobs-in-bangalore?k=data%20scientist&l=bangalore\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception Raised Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//section[@class='job-desc']\"}\n",
      "  (Session info: chrome=89.0.4389.114)\n",
      "\n",
      "Exception Raised Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//section[@class='job-desc']\"}\n",
      "  (Session info: chrome=89.0.4389.114)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extracting all the tags having the job title\n",
    "job_title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "job_title\n",
    "\n",
    "#Extracting the text from the tags\n",
    "title=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in job_title[:10]:\n",
    "    title.append(i.text)\n",
    "title    \n",
    "\n",
    "#Extracting all the tags having the job location\n",
    "job_location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "job_location\n",
    "\n",
    "#Extracting the text from the tags\n",
    "location=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in job_location[:10]:\n",
    "    location.append(i.text)\n",
    "location    \n",
    "\n",
    "#Extracting the tags having the company name\n",
    "name=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "name\n",
    "\n",
    "#Extracting the text from the tags\n",
    "company=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in name[:10]:\n",
    "    company.append(i.text)\n",
    "company\n",
    "\n",
    "#Extracting the full-job description tags from the webpage\n",
    "#First we will extract the links alone for the full job desc\n",
    "job_desc=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "job_desc\n",
    "\n",
    "#After extracting the link's tags, we will append to an empty list and extract the links\n",
    "desc=[]  #Empty list\n",
    "for i in job_desc[:10]:\n",
    "    desc.append(i.get_attribute(\"href\"))\n",
    "desc \n",
    "\n",
    "#After extracting links, we will make another for loop for extracting the text\n",
    "#Importing exception\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "job_desc=[]  #Empty list\n",
    "for i in desc:   #running a for loop for fetching the driver\n",
    "    driver.get(i)\n",
    "    time.sleep(3)   #Making the webpage sleep for 3 seconds using timer\n",
    "    try:\n",
    "        full_desc=driver.find_element_by_xpath((\"//section[@class='job-desc']\")).text  #XPath of full job description\n",
    "        job_desc.append(full_desc.replace('Job description\\n',' '))\n",
    "    except NoSuchElementException as e:\n",
    "        print(\"Exception Raised\",e)  #Printing exception message\n",
    "        job_desc.append(\"Not Available\") #Appending Not Available for no job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company name</th>\n",
       "      <th>Experience required</th>\n",
       "      <th>Job Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>CronJ IT Technologies Private Limited</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Responsibilities and Duties\\nCreate innovativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - Machine Learning</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>AugmatrixGo</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "      <td>Roles and Responsibilities\\n\\n\\n- Selecting f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist || Data Analyst || Data science</td>\n",
       "      <td>Navi Mumbai, Bangalore/Bengaluru</td>\n",
       "      <td>Inspiration Manpower Consultancy Pvt. Ltd.</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "      <td>Job Summary and Key Responsibilities\\nManage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - IBM Garage</td>\n",
       "      <td>Noida, Hyderabad/Secunderabad, Bangalore/Benga...</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBCG IND - GAMMA Senior Data Scientist</td>\n",
       "      <td>Mumbai, New Delhi, Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>1-2 Yrs</td>\n",
       "      <td>What Youll Do\\n\\nWe re looking for a pass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist/Senior Data Scientist</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Bangalo...</td>\n",
       "      <td>GANIT BUSINESS SOLUTIONS PRIVATE LIMITED</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>\\nAbout Ganit Inc\\n\\nFounded by senior indust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist | CES IT LTD | CMMI Level 5</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "      <td>CES Ltd.</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Roles and Responsibilities\\n\\nMust have stron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Associate Data Scientist - CRM &amp; Loyalty</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell India Markets Private Limited</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>The Role\\nGeneral Position Definition\\nThis r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Analyst / ML &amp; AI Engineer / Data ...</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Bangalore/Bengaluru</td>\n",
       "      <td>CHANDRA PUMPS</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>-Ability to learn new concepts quickly and tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                                     Data Scientist   \n",
       "1                  Data Scientist - Machine Learning   \n",
       "2     Data Scientist || Data Analyst || Data science   \n",
       "3                        Data Scientist - IBM Garage   \n",
       "4             DBCG IND - GAMMA Senior Data Scientist   \n",
       "5               Data Scientist/Senior Data Scientist   \n",
       "6  Senior Data Scientist | CES IT LTD | CMMI Level 5   \n",
       "7           Associate Data Scientist - CRM & Loyalty   \n",
       "8                                     Data Scientist   \n",
       "9  Senior Data Analyst / ML & AI Engineer / Data ...   \n",
       "\n",
       "                                            Location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                   Navi Mumbai, Bangalore/Bengaluru   \n",
       "3  Noida, Hyderabad/Secunderabad, Bangalore/Benga...   \n",
       "4    Mumbai, New Delhi, Chennai, Bangalore/Bengaluru   \n",
       "5  Hyderabad/Secunderabad, Pune, Chennai, Bangalo...   \n",
       "6  Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9  Hyderabad/Secunderabad, Pune, Bangalore/Bengaluru   \n",
       "\n",
       "                                 Company name Experience required  \\\n",
       "0       CronJ IT Technologies Private Limited             0-2 Yrs   \n",
       "1                                 AugmatrixGo             6-8 Yrs   \n",
       "2  Inspiration Manpower Consultancy Pvt. Ltd.             0-0 Yrs   \n",
       "3                      IBM India Pvt. Limited             0-2 Yrs   \n",
       "4                     Boston Consulting Group             1-2 Yrs   \n",
       "5    GANIT BUSINESS SOLUTIONS PRIVATE LIMITED             2-5 Yrs   \n",
       "6                                    CES Ltd.             3-6 Yrs   \n",
       "7         Shell India Markets Private Limited             4-8 Yrs   \n",
       "8                      IBM India Pvt. Limited             3-8 Yrs   \n",
       "9                               CHANDRA PUMPS             4-8 Yrs   \n",
       "\n",
       "                                     Job Description  \n",
       "0   Responsibilities and Duties\\nCreate innovativ...  \n",
       "1   Roles and Responsibilities\\n\\n\\n- Selecting f...  \n",
       "2    Job Summary and Key Responsibilities\\nManage...  \n",
       "3                                      Not Available  \n",
       "4       What Youll Do\\n\\nWe re looking for a pass...  \n",
       "5   \\nAbout Ganit Inc\\n\\nFounded by senior indust...  \n",
       "6   Roles and Responsibilities\\n\\nMust have stron...  \n",
       "7   The Role\\nGeneral Position Definition\\nThis r...  \n",
       "8                                      Not Available  \n",
       "9   -Ability to learn new concepts quickly and tr...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking out the length of the data extracted\n",
    "print(len(title),len(location),len(company),len(exp),len(job_desc))\n",
    "\n",
    "#Creating a new dataframe for saving the data\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['Title']=title\n",
    "jobs['Location']=location\n",
    "jobs['Company name']=company\n",
    "jobs['Experience required']=exp\n",
    "jobs['Job Description']=job_desc\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You have to use the location and salary filter.\n",
    "* You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "* You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "* The location filter to be used is “Delhi/NCR”\n",
    "* The salary filter to be used is “3-6” lakhs\n",
    "\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(r\"D://chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.naukri.com/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the required elements from the search bar of job\n",
    "job_search=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "\n",
    "#Sending the input to the webpage\n",
    "job_search.send_keys(\"Data Scientist\")\n",
    "\n",
    "#Searching the input by using the search button and clicking it\n",
    "driver.find_element_by_xpath(\"//div[@class='search-btn']/button\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locating the filters in the webpage\n",
    "#First, we will filter the location\n",
    "filter_location=driver.find_elements_by_xpath(\"//span[@class='ellipsis fleft']\")\n",
    "for i in filter_location:\n",
    "    if i.text=='Delhi / NCR':\n",
    "        i.click()\n",
    "        break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the salary\n",
    "filter_salary=driver.find_elements_by_xpath(\"//span[@class='ellipsis fleft']\")\n",
    "for i in filter_salary:\n",
    "    if i.text=='3-6 Lakhs':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After filtering location and salary, we will specify the url of webpage\n",
    "url=\"https://www.naukri.com/data-scientist-jobs?k=data%20scientist&ctcFilter=3to6&cityTypeGid=9508\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company name</th>\n",
       "      <th>Experience required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - IBM Garage</td>\n",
       "      <td>Noida, Hyderabad/Secunderabad, Bangalore/Benga...</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Females Required- Data Scientist- Noida</td>\n",
       "      <td>Noida, Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>Randstad</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - Python &amp; Machine Learning</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Bangalore/Bengal...</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Python &amp; Machine Learning</td>\n",
       "      <td>Hyderabad/Secunderabad, Pune, Chennai, Bangalo...</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - Hadoop/BigQuery</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Jubna</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Python / Machine Learning / T...</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Bangalore/Beng...</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - Python &amp; Machine Learning</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, D...</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Required- Data Scientist (NLP)-Axis Bank - 6 m...</td>\n",
       "      <td>Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...</td>\n",
       "      <td>Axis Bank Limited</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist - Python / Machine Learning / T...</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, D...</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist - Python &amp; Machine Learning (IS...</td>\n",
       "      <td>Hyderabad/Secunderabad, Bangalore/Bengaluru, D...</td>\n",
       "      <td>FUTURES AND CAREERS</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                        Data Scientist - IBM Garage   \n",
       "1            Females Required- Data Scientist- Noida   \n",
       "2         Data Scientist - Python & Machine Learning   \n",
       "3         Data Scientist - Python & Machine Learning   \n",
       "4                   Data Scientist - Hadoop/BigQuery   \n",
       "5  Data Scientist - Python / Machine Learning / T...   \n",
       "6         Data Scientist - Python & Machine Learning   \n",
       "7  Required- Data Scientist (NLP)-Axis Bank - 6 m...   \n",
       "8  Data Scientist - Python / Machine Learning / T...   \n",
       "9  Data Scientist - Python & Machine Learning (IS...   \n",
       "\n",
       "                                            Location            Company name  \\\n",
       "0  Noida, Hyderabad/Secunderabad, Bangalore/Benga...  IBM India Pvt. Limited   \n",
       "1               Noida, Gurgaon/Gurugram, Delhi / NCR                Randstad   \n",
       "2  Hyderabad/Secunderabad, Pune, Bangalore/Bengal...     FUTURES AND CAREERS   \n",
       "3  Hyderabad/Secunderabad, Pune, Chennai, Bangalo...     FUTURES AND CAREERS   \n",
       "4                                              Noida                   Jubna   \n",
       "5  Mumbai, Hyderabad/Secunderabad, Bangalore/Beng...     FUTURES AND CAREERS   \n",
       "6  Hyderabad/Secunderabad, Bangalore/Bengaluru, D...     FUTURES AND CAREERS   \n",
       "7  Kolkata, New Delhi, Hyderabad/Secunderabad, Pu...       Axis Bank Limited   \n",
       "8  Hyderabad/Secunderabad, Bangalore/Bengaluru, D...     FUTURES AND CAREERS   \n",
       "9  Hyderabad/Secunderabad, Bangalore/Bengaluru, D...     FUTURES AND CAREERS   \n",
       "\n",
       "  Experience required  \n",
       "0             5-8 Yrs  \n",
       "1             3-7 Yrs  \n",
       "2             2-7 Yrs  \n",
       "3             2-7 Yrs  \n",
       "4             3-6 Yrs  \n",
       "5             3-8 Yrs  \n",
       "6             2-7 Yrs  \n",
       "7             4-9 Yrs  \n",
       "8             3-8 Yrs  \n",
       "9             3-8 Yrs  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting all the tags having the job title\n",
    "job_title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "job_title\n",
    "\n",
    "#Extracting the text from the tags\n",
    "title=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in job_title[:10]:\n",
    "    title.append(i.text)\n",
    "title   \n",
    "\n",
    "#Extracting all the tags having the job location\n",
    "job_location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "job_location\n",
    "\n",
    "#Extracting the text from the tags\n",
    "location=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in job_location[:10]:\n",
    "    location.append(i.text)\n",
    "location    \n",
    "\n",
    "#Extracting the tags having the company name\n",
    "name=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "name\n",
    "\n",
    "#Extracting the text from the tags\n",
    "company=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in name[:10]:\n",
    "    company.append(i.text)\n",
    "company\n",
    "\n",
    "#Extracting the tags having experience required\n",
    "exp_required=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span\")\n",
    "exp_required\n",
    "\n",
    "#Extracting the text from the tags\n",
    "exp=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in exp_required[:10]:\n",
    "    exp.append(i.text)\n",
    "exp\n",
    "\n",
    "#Checking out the length of the data extracted\n",
    "print(len(title),len(location),len(company),len(exp))\n",
    "\n",
    "#Creating a new dataframe for saving the data\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['Title']=title\n",
    "jobs['Location']=location\n",
    "jobs['Company name']=company\n",
    "jobs['Experience required']=exp\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida” in “location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown page.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(r\"D://chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.glassdoor.co.in/index.htm')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the required elements from the search bars of job and location\n",
    "job_search=driver.find_element_by_id('sc.keyword')\n",
    "location_search=driver.find_element_by_id('sc.location')\n",
    "\n",
    "#Sending the inputs to the webpage\n",
    "job_search.send_keys(\"Data Scientist\")\n",
    "location_search.send_keys(\"Noida\")\n",
    "\n",
    "#Searching the inputs by using the search button and clicking it\n",
    "driver.find_element_by_xpath(\"//button[@class='gd-ui-button ml-std col-auto SearchStyles__newSearchButton css-iixdfr']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the url of the webpage to be scraped\n",
    "url=\"https://www.glassdoor.co.in/Job/noida-data-scientist-jobs-SRCH_IL.0,5_IC4477468_KO6,20.htm\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company name</th>\n",
       "      <th>Days posted</th>\n",
       "      <th>Rating of the company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BlackRock</td>\n",
       "      <td>20 days ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BlackRock</td>\n",
       "      <td>20 days ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gauge Data Solutions</td>\n",
       "      <td>24h</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adobe</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Healtheoz India</td>\n",
       "      <td>24h</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Priority Vendor</td>\n",
       "      <td>24h</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SearchUrCollege</td>\n",
       "      <td>24h</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abc consultants</td>\n",
       "      <td>1 days ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Biz2Credit Inc</td>\n",
       "      <td>9 days ago</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Techlive</td>\n",
       "      <td>7 days ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Company name  Days posted Rating of the company\n",
       "0             BlackRock  20 days ago                   4.0\n",
       "1             BlackRock  20 days ago                   4.0\n",
       "2  Gauge Data Solutions          24h                   3.1\n",
       "3                 Adobe   3 days ago                   4.4\n",
       "4       Healtheoz India          24h                   4.8\n",
       "5       Priority Vendor          24h                   3.7\n",
       "6       SearchUrCollege          24h                   4.2\n",
       "7       abc consultants   1 days ago                   3.8\n",
       "8        Biz2Credit Inc   9 days ago                   5.0\n",
       "9              Techlive   7 days ago                   4.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting the tags having the company name\n",
    "name=driver.find_elements_by_xpath(\"//div[@class='d-flex justify-content-between align-items-start']/a/span\")\n",
    "name\n",
    "\n",
    "#Extracting the text from the tags\n",
    "company=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in name[:10]:\n",
    "    company.append(i.text)\n",
    "company\n",
    "\n",
    "#Extracting the tags having the no of days when job was posted\n",
    "days=driver.find_elements_by_xpath(\"//div[@class='d-flex align-items-end pl-std css-mi55ob']\")\n",
    "days\n",
    "\n",
    "#Extracting the text from the tags\n",
    "no_of_days=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in days[:10]:\n",
    "    no_of_days.append(i.text.replace('d',' days ago'))\n",
    "no_of_days\n",
    "\n",
    "#Extracting the tags having the rating of the company\n",
    "rating=driver.find_elements_by_xpath(\"//span[@class='css-19pjha7 e1cjmv6j1']\")\n",
    "rating\n",
    "\n",
    "#Extracting the text from the tags\n",
    "ratings=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in rating[:10]:\n",
    "    ratings.append(i.text)\n",
    "ratings\n",
    "\n",
    "#Checking out the length of the data extracted\n",
    "print(len(company),len(no_of_days),len(ratings))\n",
    "\n",
    "#Creating a new dataframe for saving the data\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['Company name']=company\n",
    "jobs['Days posted']=no_of_days\n",
    "jobs['Rating of the company']=ratings\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location.You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3. Click the search button.\n",
    "4. After that you will land on the page.\n",
    "5. You have to scrape whole data from this webpage\n",
    "6. Scrape data for first 10 companies. Scrape the min salary, max salary, company name, Average salary and rating of the company.\n",
    "7. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(r\"D://chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.glassdoor.co.in/Salaries/index.htm')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the required elements from the search bars of job and location\n",
    "job_search=driver.find_element_by_id('KeywordSearch')\n",
    "location_search=driver.find_element_by_id('LocationSearch')\n",
    "\n",
    "#Sending the inputs to the webpage\n",
    "job_search.send_keys(\"Data Scientist\")\n",
    "location_search.send_keys(\"Noida\")\n",
    "\n",
    "#Searching the inputs by using the search button and clicking it\n",
    "driver.find_element_by_xpath(\"//button[@class='gd-btn-mkt']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the url of the webpage to be scraped\n",
    "url=\"https://www.glassdoor.co.in/Salaries/new-delhi-data-scientist-salary-SRCH_IL.0,9_IM1083_KO10,24.htm?clickSource=searchBtn\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Number of salaries</th>\n",
       "      <th>Average salary</th>\n",
       "      <th>Minimum salary</th>\n",
       "      <th>Maximum salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>14 salaries</td>\n",
       "      <td>₹ 5,97,967/yr</td>\n",
       "      <td>₹333K</td>\n",
       "      <td>₹1,080K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>14 salaries</td>\n",
       "      <td>₹ 11,12,243/yr</td>\n",
       "      <td>₹560K</td>\n",
       "      <td>₹2,147K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhivery</td>\n",
       "      <td>14 salaries</td>\n",
       "      <td>₹ 12,12,741/yr</td>\n",
       "      <td>₹436K</td>\n",
       "      <td>₹11,274K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IBM</td>\n",
       "      <td>13 salaries</td>\n",
       "      <td>₹ 7,37,972/yr</td>\n",
       "      <td>₹569K</td>\n",
       "      <td>₹2,648K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>12 salaries</td>\n",
       "      <td>₹ 7,15,984/yr</td>\n",
       "      <td>₹350K</td>\n",
       "      <td>₹1,565K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>10 salaries</td>\n",
       "      <td>₹ 13,41,900/yr</td>\n",
       "      <td>₹1,037K</td>\n",
       "      <td>₹1,500K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>9 salaries</td>\n",
       "      <td>₹ 7,90,812/yr</td>\n",
       "      <td>₹487K</td>\n",
       "      <td>₹1,421K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Innovaccer</td>\n",
       "      <td>8 salaries</td>\n",
       "      <td>₹ 11,81,047/yr</td>\n",
       "      <td>₹602K</td>\n",
       "      <td>₹1,644K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZS Associates</td>\n",
       "      <td>7 salaries</td>\n",
       "      <td>₹ 9,89,924/yr</td>\n",
       "      <td>₹196K</td>\n",
       "      <td>₹1,755K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EXL Service</td>\n",
       "      <td>7 salaries</td>\n",
       "      <td>₹ 11,73,127/yr</td>\n",
       "      <td>₹558K</td>\n",
       "      <td>₹1,500K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Company Name Number of salaries  Average salary  \\\n",
       "0  Tata Consultancy Services        14 salaries   ₹ 5,97,967/yr   \n",
       "1                  Accenture        14 salaries  ₹ 11,12,243/yr   \n",
       "2                  Delhivery        14 salaries  ₹ 12,12,741/yr   \n",
       "3                        IBM        13 salaries   ₹ 7,37,972/yr   \n",
       "4         Ericsson-Worldwide        12 salaries   ₹ 7,15,984/yr   \n",
       "5         UnitedHealth Group        10 salaries  ₹ 13,41,900/yr   \n",
       "6         Valiance Solutions         9 salaries   ₹ 7,90,812/yr   \n",
       "7                 Innovaccer         8 salaries  ₹ 11,81,047/yr   \n",
       "8              ZS Associates         7 salaries   ₹ 9,89,924/yr   \n",
       "9                EXL Service         7 salaries  ₹ 11,73,127/yr   \n",
       "\n",
       "  Minimum salary Maximum salary  \n",
       "0          ₹333K        ₹1,080K  \n",
       "1          ₹560K        ₹2,147K  \n",
       "2          ₹436K       ₹11,274K  \n",
       "3          ₹569K        ₹2,648K  \n",
       "4          ₹350K        ₹1,565K  \n",
       "5        ₹1,037K        ₹1,500K  \n",
       "6          ₹487K        ₹1,421K  \n",
       "7          ₹602K        ₹1,644K  \n",
       "8          ₹196K        ₹1,755K  \n",
       "9          ₹558K        ₹1,500K  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting the tags having the company name\n",
    "comp_name=driver.find_elements_by_xpath(\"//div[@class='d-flex']/div[2]/p[2]\")\n",
    "comp_name\n",
    "\n",
    "#Extracting the text from the tags\n",
    "company=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in comp_name[:10]:\n",
    "    company.append(i.text)\n",
    "company\n",
    "\n",
    "#Extracting the tags having the number of salaries\n",
    "no_salaries=driver.find_elements_by_xpath(\"//div[@class='d-flex']/div[2]/p[5]\")\n",
    "no_salaries\n",
    "\n",
    "#Extracting the text from the tags\n",
    "no_of_salaries=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in no_salaries[:10]:\n",
    "    no_of_salaries.append(i.text)\n",
    "no_of_salaries\n",
    "\n",
    "#Extracting the tags having average salary\n",
    "avg_sal=driver.find_elements_by_xpath(\"//div[@class='col-2 d-none d-md-flex flex-row justify-content-end']\")\n",
    "avg_sal\n",
    "\n",
    "#Extracting the text from the tags\n",
    "avg_salary=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in avg_sal[:10]:\n",
    "    avg_salary.append(i.text.replace('\\n',''))\n",
    "avg_salary\n",
    "\n",
    "#Extracting the tags having minimum salary\n",
    "min_sal=driver.find_elements_by_xpath(\"//div[@class='col-3 offset-1 d-none d-md-block']/div/div[2]/span[1]\")\n",
    "min_sal\n",
    "\n",
    "#Extracting the text from the tags\n",
    "min_salary=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in min_sal[:10]:\n",
    "    min_salary.append(i.text)\n",
    "min_salary\n",
    "\n",
    "#Extracting the tags having the maximum salary\n",
    "max_sal=driver.find_elements_by_xpath(\"//div[@class='col-3 offset-1 d-none d-md-block']/div/div[2]/span[2]\")\n",
    "max_sal\n",
    "\n",
    "#Extracting the text from the tags\n",
    "max_salary=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 job results, we are running a for loop for first 10 results only\n",
    "for i in max_sal[:10]:\n",
    "    max_salary.append(i.text)\n",
    "max_salary\n",
    "\n",
    "#Checking out the length of the data extracted\n",
    "print(len(company),len(no_of_salaries),len(avg_salary),len(min_salary),len(max_salary))\n",
    "\n",
    "#Creating a new dataframe for saving the data\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['Company Name']=company\n",
    "jobs['Number of salaries']=no_of_salaries\n",
    "jobs['Average salary']=avg_salary\n",
    "jobs['Minimum salary']=min_salary\n",
    "jobs['Maximum salary']=max_salary\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6 : Scrape data of first 100 sunglasses listings on flipkart.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to scrape four attributes:\n",
    "   1. Brand\n",
    "   2. Product Description\n",
    "   3. Price\n",
    "   4. Discount %\n",
    "   \n",
    "To scrape the data you have to go through following steps:\n",
    "\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon.\n",
    "3. After that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "7. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(r\"D://chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get(' https://www.flipkart.com/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching sunglasses in the search bar and clicking the search button\n",
    "search_bar=driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "search_bar.send_keys('sunglasses')\n",
    "\n",
    "driver.find_element_by_xpath(\"//button[@type='submit']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the url of the webpage to be scraped\n",
    "url=\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the empty lists\n",
    "brand=[]\n",
    "prod_desc=[]\n",
    "org_price=[]\n",
    "discount=[]\n",
    "disc_price=[]\n",
    "\n",
    "#We need to scrap for 100 products and here we will consider the data from first 3 pages\n",
    "#We will take a for loop and scrap data from all the pages\n",
    "for i in range(0,3):\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\"):\n",
    "        brand.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\"):\n",
    "        prod_desc.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3I9_wc']\"):\n",
    "        org_price.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span\"):\n",
    "        discount.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\"):\n",
    "        disc_price.append(j.text)\n",
    "   \n",
    "    #Path for next page as it changes for every page. We are appending numbers as pages change  \n",
    "    k=i+1\n",
    "    next_page=\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=\"+str(k)\n",
    "    driver.get(next_page)\n",
    "    time.sleep(2)  #2 seconds wait time for each page    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120 120 120 120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Discounted Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Mirrored Aviator Sunglasses (58)</td>\n",
       "      <td>₹1,499</td>\n",
       "      <td>73% off</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹1,599</td>\n",
       "      <td>84% off</td>\n",
       "      <td>₹250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹899</td>\n",
       "      <td>15% off</td>\n",
       "      <td>₹758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹799</td>\n",
       "      <td>16% off</td>\n",
       "      <td>₹666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>₹1,599</td>\n",
       "      <td>84% off</td>\n",
       "      <td>₹250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (56)</td>\n",
       "      <td>₹799</td>\n",
       "      <td>15% off</td>\n",
       "      <td>₹679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection, Night Vision, Riding Glasses Sp...</td>\n",
       "      <td>₹945</td>\n",
       "      <td>77% off</td>\n",
       "      <td>₹209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹1,499</td>\n",
       "      <td>73% off</td>\n",
       "      <td>₹398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>Gradient, UV Protection Round Sunglasses (Free...</td>\n",
       "      <td>₹1,995</td>\n",
       "      <td>77% off</td>\n",
       "      <td>₹449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Aislin</td>\n",
       "      <td>UV Protection, Gradient Wayfarer Sunglasses (57)</td>\n",
       "      <td>₹2,150</td>\n",
       "      <td>68% off</td>\n",
       "      <td>₹683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                        Description  \\\n",
       "0        ROYAL SON                   Mirrored Aviator Sunglasses (58)   \n",
       "1           PIRASO              UV Protection Aviator Sunglasses (54)   \n",
       "2         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)   \n",
       "3         Fastrack   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "4           PIRASO              UV Protection Aviator Sunglasses (54)   \n",
       "..             ...                                                ...   \n",
       "95        Fastrack          UV Protection Rectangular Sunglasses (56)   \n",
       "96           NuVew  UV Protection, Night Vision, Riding Glasses Sp...   \n",
       "97  ROZZETTA CRAFT   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "98  ROZZETTA CRAFT  Gradient, UV Protection Round Sunglasses (Free...   \n",
       "99          Aislin   UV Protection, Gradient Wayfarer Sunglasses (57)   \n",
       "\n",
       "   Original Price Discount Discounted Price  \n",
       "0          ₹1,499  73% off             ₹399  \n",
       "1          ₹1,599  84% off             ₹250  \n",
       "2            ₹899  15% off             ₹758  \n",
       "3            ₹799  16% off             ₹666  \n",
       "4          ₹1,599  84% off             ₹250  \n",
       "..            ...      ...              ...  \n",
       "95           ₹799  15% off             ₹679  \n",
       "96           ₹945  77% off             ₹209  \n",
       "97         ₹1,499  73% off             ₹398  \n",
       "98         ₹1,995  77% off             ₹449  \n",
       "99         ₹2,150  68% off             ₹683  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the length of the data scraped\n",
    "print(len(brand),len(prod_desc),len(org_price),len(discount),len(disc_price))\n",
    "\n",
    "#Creating a new dataframe\n",
    "glass=pd.DataFrame({})\n",
    "glass['Brand']=brand[:100]\n",
    "glass['Description']=prod_desc[:100]\n",
    "glass['Original Price']=org_price[:100]\n",
    "glass['Discount']=discount[:100]\n",
    "glass['Discounted Price']=disc_price[:100]\n",
    "glass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\n",
    "2. When you will open the above link you will reach to the webpage.\n",
    "3. As shown in the above page you have to scrape the following attributes:\n",
    "   1. Rating\n",
    "   2. Review_summary\n",
    "   3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.  \n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(r\"D://chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 110 110\n"
     ]
    }
   ],
   "source": [
    "#Taking the empty lists\n",
    "Rating=[]\n",
    "review=[]\n",
    "full_summary=[]\n",
    "\n",
    "#As there are nearly 10 reviews per page, we will check for 11 pages and scrap the required data\n",
    "#Now we will take a for loop and scrap\n",
    "for i in range(0,11):\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\"):\n",
    "        Rating.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\"):\n",
    "        review.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\"):\n",
    "        full_summary.append(j.text)\n",
    "        \n",
    "    #Path for next page as it changes for every page. We are appending numbers as pages change  \n",
    "    k=i+1\n",
    "    next_page=\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=\"+str(k) \n",
    "    driver.get(next_page)\n",
    "    time.sleep(2)  #2 seconds wait time for each page\n",
    "    \n",
    "#Checking the length of the data scraped\n",
    "print(len(Rating),len(review),len(full_summary))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Full summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>Awesome</td>\n",
       "      <td>So good look comfortable iPhone 11 I am so ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Good choice</td>\n",
       "      <td>I have my first iPhone\\nI love it\\nusing 15 da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>I upgraded from one plus 6 to iPhone 11 and it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Classy product</td>\n",
       "      <td>This is very second time I am using iphone I a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>I’d like to start by saying that the overall e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating              Review  \\\n",
       "0       5           Brilliant   \n",
       "1       5    Perfect product!   \n",
       "2       5   Worth every penny   \n",
       "3       5       Great product   \n",
       "4       5  Highly recommended   \n",
       "..    ...                 ...   \n",
       "95      4             Awesome   \n",
       "96      5         Good choice   \n",
       "97      5   Worth every penny   \n",
       "98      5      Classy product   \n",
       "99      5              Super!   \n",
       "\n",
       "                                         Full summary  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Previously I was using one plus 3t it was a gr...  \n",
       "3   Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "4   iphone 11 is a very good phone to buy only if ...  \n",
       "..                                                ...  \n",
       "95  So good look comfortable iPhone 11 I am so ver...  \n",
       "96  I have my first iPhone\\nI love it\\nusing 15 da...  \n",
       "97  I upgraded from one plus 6 to iPhone 11 and it...  \n",
       "98  This is very second time I am using iphone I a...  \n",
       "99  I’d like to start by saying that the overall e...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a new dataframe\n",
    "phone=pd.DataFrame({})\n",
    "phone['Rating']=Rating[:100]\n",
    "phone['Review']=review[:100]\n",
    "phone['Full summary']=full_summary[:100]\n",
    "phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(r\"D://chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get(' https://www.flipkart.com/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching sneakers in the search bar and clicking the search button\n",
    "search_bar=driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "search_bar.send_keys('sneakers')\n",
    "\n",
    "driver.find_element_by_xpath(\"//button[@type='submit']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the url of the webpage to be scraped\n",
    "url=\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 131 156 156 160\n"
     ]
    }
   ],
   "source": [
    "#Taking the empty lists\n",
    "brand=[]\n",
    "prod_desc=[]\n",
    "org_price=[]\n",
    "discount=[]\n",
    "disc_price=[]\n",
    "\n",
    "#We need to scrap for 100 products and here we will consider the data from first 4 pages\n",
    "#We will take a for loop and scrap data from all the pages\n",
    "for i in range(0,4):\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\"):\n",
    "        brand.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\"):\n",
    "        prod_desc.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3I9_wc']\"):\n",
    "        org_price.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']/span\"):\n",
    "        discount.append(j.text)\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\"):\n",
    "        disc_price.append(j.text)\n",
    "   \n",
    "    #Path for next page as it changes for every page. We are appending numbers as pages change  \n",
    "    k=i+1\n",
    "    next_page=\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=\"+str(k)\n",
    "    driver.get(next_page)\n",
    "    time.sleep(2)  #2 seconds wait time for each page\n",
    "    \n",
    "#Checking the length of the data scraped\n",
    "print(len(brand),len(prod_desc),len(org_price),len(discount),len(disc_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Original Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Discounted Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Perfect &amp; Affordable Combo Pack of 02 Pairs Sn...</td>\n",
       "      <td>₹1,798</td>\n",
       "      <td>72% off</td>\n",
       "      <td>₹499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers Shoes For Men Sneakers For Men</td>\n",
       "      <td>₹999</td>\n",
       "      <td>60% off</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Super &amp; Trendy Men's Pack of 02 Pair Shoes for...</td>\n",
       "      <td>₹1,998</td>\n",
       "      <td>78% off</td>\n",
       "      <td>₹420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹1,996</td>\n",
       "      <td>76% off</td>\n",
       "      <td>₹474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shoes Bank</td>\n",
       "      <td>White Sneaker For Men's/Boy's Sneakers For Men</td>\n",
       "      <td>₹999</td>\n",
       "      <td>65% off</td>\n",
       "      <td>₹349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ktiz</td>\n",
       "      <td>168 Smart Red Lace-Ups Casuals for Men Sneaker...</td>\n",
       "      <td>₹1,355</td>\n",
       "      <td>79% off</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Sneakers Sneakers For Men</td>\n",
       "      <td>₹999</td>\n",
       "      <td>30% off</td>\n",
       "      <td>₹594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>believe</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "      <td>60% off</td>\n",
       "      <td>₹446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ROCKFIELD</td>\n",
       "      <td>Synthetic Leather Casual Sneakers Shoes For Me...</td>\n",
       "      <td>₹999</td>\n",
       "      <td>63% off</td>\n",
       "      <td>₹424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Axter</td>\n",
       "      <td>Super 445 Fashion Sneakers For Men</td>\n",
       "      <td>₹999</td>\n",
       "      <td>30% off</td>\n",
       "      <td>₹284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Brand                                        Description  \\\n",
       "0         Chevit  Perfect & Affordable Combo Pack of 02 Pairs Sn...   \n",
       "1   Robbie jones     Casual Sneakers Shoes For Men Sneakers For Men   \n",
       "2         Chevit  Super & Trendy Men's Pack of 02 Pair Shoes for...   \n",
       "3         Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...   \n",
       "4     Shoes Bank     White Sneaker For Men's/Boy's Sneakers For Men   \n",
       "..           ...                                                ...   \n",
       "95          Ktiz  168 Smart Red Lace-Ups Casuals for Men Sneaker...   \n",
       "96        Chevit                          Sneakers Sneakers For Men   \n",
       "97       believe                                   Sneakers For Men   \n",
       "98     ROCKFIELD  Synthetic Leather Casual Sneakers Shoes For Me...   \n",
       "99         Axter                 Super 445 Fashion Sneakers For Men   \n",
       "\n",
       "   Original Price Discount Discounted Price  \n",
       "0          ₹1,798  72% off             ₹499  \n",
       "1            ₹999  60% off             ₹399  \n",
       "2          ₹1,998  78% off             ₹420  \n",
       "3          ₹1,996  76% off             ₹474  \n",
       "4            ₹999  65% off             ₹349  \n",
       "..            ...      ...              ...  \n",
       "95         ₹1,355  79% off             ₹399  \n",
       "96           ₹999  30% off             ₹594  \n",
       "97           ₹499  60% off             ₹446  \n",
       "98           ₹999  63% off             ₹424  \n",
       "99           ₹999  30% off             ₹284  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a new dataframe\n",
    "sneakers=pd.DataFrame({})\n",
    "sneakers['Brand']=brand[:100]\n",
    "sneakers['Description']=prod_desc[:100]\n",
    "sneakers['Original Price']=org_price[:100]\n",
    "sneakers['Discount']=discount[:100]\n",
    "sneakers['Discounted Price']=disc_price[:100]\n",
    "sneakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9: Myntra webpage scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to the link - https://www.myntra.com/shoes\n",
    "2. Set Price filter to “Rs. 6649 to Rs. 13099” and Color filter to “Black”, as shown in the image.\n",
    "3. And then scrape First 100 shoes data you get.\n",
    "4. The data should include “Brand” of the shoes , Short Shoe description and price of the shoes.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(r\"D://chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.myntra.com/shoes')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the color black\n",
    "driver.find_element_by_xpath(\"//span[@data-colorhex='black']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the price range Rs. 6649 to Rs. 13099\n",
    "driver.find_element_by_xpath(\"//ul[@class='price-list']/li[2]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After filtering price range and color, we will specify the url of webpage\n",
    "url=\"https://www.myntra.com/shoes?f=Color%3ABlack_36454f&plaEnabled=false&rf=Price%3A6657.0_13105.0_6657.0%20TO%2013105.0\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "#Taking the empty lists\n",
    "brand=[]\n",
    "short_desc=[]\n",
    "price=[]\n",
    "\n",
    "#We can see that there are 50 products in one page, and as we need to scrap for 100 products, we need to take 2 pages data\n",
    "for i in range(0,2):\n",
    "    #Finding the tags having the brand name\n",
    "    for j in driver.find_elements_by_xpath(\"//h3[@class='product-brand']\"):\n",
    "        brand.append(j.text) #Extracting text from tags and appending to emptylist\n",
    "    \n",
    "    #Finding the tags having short description of the shoe\n",
    "    for j in driver.find_elements_by_xpath(\"//h4[@class='product-product']\"):\n",
    "        short_desc.append(j.text) #Extracting text from tags and appending to emptylist\n",
    "        \n",
    "    #Finding the tags having the price of the shoe\n",
    "    for j in driver.find_elements_by_xpath(\"//div[@class='product-price']\"):\n",
    "        price.append(j.text)  #Extracting text from tags and appending to emptylist\n",
    "    driver.find_element_by_class_name('pagination-next').click()   #Button path for moving to next page\n",
    "    time.sleep(2)  #2 seconds wait time\n",
    "    \n",
    "#Checking the length of the list\n",
    "print(len(brand),len(short_desc),len(price))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand name</th>\n",
       "      <th>Short description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Solid Leather Formal Slip-Ons</td>\n",
       "      <td>Rs. 8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Solid Leather Formal Slip-Ons</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>Men Leather Formal Oxfords</td>\n",
       "      <td>Rs. 7693Rs. 10990(30% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>Men Leather Formal Loafers</td>\n",
       "      <td>Rs. 7992Rs. 9990(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Formal Derbys</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Formal Derbys</td>\n",
       "      <td>Rs. 9742Rs. 12990(25% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Women Leather Boat Shoes</td>\n",
       "      <td>Rs. 8990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Women Solid Leather Flats</td>\n",
       "      <td>Rs. 7492Rs. 9990(25% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Formal Oxfords</td>\n",
       "      <td>Rs. 7867Rs. 10490(25% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Formal Oxfords</td>\n",
       "      <td>Rs. 9367Rs. 12490(25% OFF)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Brand name                  Short description  \\\n",
       "0           Hush Puppies  Men Solid Leather Formal Slip-Ons   \n",
       "1           Hush Puppies  Men Solid Leather Formal Slip-Ons   \n",
       "2   Heel & Buckle London         Men Leather Formal Oxfords   \n",
       "3   Heel & Buckle London         Men Leather Formal Loafers   \n",
       "4           Hush Puppies                  Men Formal Derbys   \n",
       "..                   ...                                ...   \n",
       "95                  Geox          Men Leather Formal Derbys   \n",
       "96                  Geox           Women Leather Boat Shoes   \n",
       "97                  Geox          Women Solid Leather Flats   \n",
       "98                  Geox         Men Leather Formal Oxfords   \n",
       "99                  Geox         Men Leather Formal Oxfords   \n",
       "\n",
       "                         Price  \n",
       "0                     Rs. 8999  \n",
       "1                     Rs. 9999  \n",
       "2   Rs. 7693Rs. 10990(30% OFF)  \n",
       "3    Rs. 7992Rs. 9990(20% OFF)  \n",
       "4                     Rs. 9999  \n",
       "..                         ...  \n",
       "95  Rs. 9742Rs. 12990(25% OFF)  \n",
       "96                    Rs. 8990  \n",
       "97   Rs. 7492Rs. 9990(25% OFF)  \n",
       "98  Rs. 7867Rs. 10490(25% OFF)  \n",
       "99  Rs. 9367Rs. 12490(25% OFF)  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the length of the list\n",
    "print(len(brand),len(short_desc),len(price))\n",
    "\n",
    "#Creating a new dataframe\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand name']=brand\n",
    "shoes['Short description']=short_desc\n",
    "shoes['Price']=price\n",
    "shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10. Amazon.com webpage scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to webpage https://www.amazon.in/\n",
    "2. Enter “Laptop” in the search field and then click the search icon.\n",
    "3. Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the image.\n",
    "4. After setting the filters scrape first 10 laptops data.\n",
    "5. You have to scrape 3 attributes for each laptop:\n",
    "   1. title\n",
    "   2. Ratings\n",
    "   3. Price\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing exception\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to web driver\n",
    "driver=webdriver.Chrome(r\"D://chromedriver.exe\")  #r converts string to raw string\n",
    "#If not r, we can use executable_path = \"C:/path name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the website to driver\n",
    "driver.get('https://www.amazon.in/')\n",
    "\n",
    "#When we run this line, automatically the webpage will be opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching laptop in the search bar and clicking the search button\n",
    "search_bar=driver.find_element_by_id('twotabsearchtextbox')\n",
    "search_bar.send_keys(\"laptop\")\n",
    "\n",
    "driver.find_element_by_id('nav-search-submit-button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locating the filters from the webpage\n",
    "#Filtering Intel Core i7 from filters\n",
    "filter1=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in filter1:\n",
    "    if i.text=='Intel Core i7':\n",
    "        i.click()\n",
    "        break         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering Intel Core i9 from filters\n",
    "filter1=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in filter1:\n",
    "    if i.text=='Intel Core i9':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying the url of the webpage to be scraped\n",
    "url=\"https://www.amazon.in/s?k=laptop&i=computers&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031%7C16757432031&dc&qid=1617789939&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_17\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "#Extracting the tags having the product title\n",
    "title=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "title\n",
    "\n",
    "#Extracting the text from the tags\n",
    "prod_title=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 product results, we are running a for loop for first 10 results only\n",
    "for i in title[:10]:\n",
    "    prod_title.append(i.text)\n",
    "prod_title    \n",
    "\n",
    "#Extracting the tags having the price of the product\n",
    "price=driver.find_elements_by_xpath(\"//span[@class='a-price']\")\n",
    "price\n",
    "\n",
    "#Extracting the text from the tags\n",
    "prod_price=[]  #Empty list\n",
    "\n",
    "#As we need to scrap data for the first 10 product results, we are running a for loop for first 10 results only\n",
    "for i in price[:10]:\n",
    "    prod_price.append(i.text)\n",
    "prod_price\n",
    "\n",
    "#Extracting the tags having the product ratings\n",
    "#First we will collect the urls of all laptops\n",
    "laptop_urls=driver.find_elements_by_xpath(\"//a[@class='a-link-normal a-text-normal']\")\n",
    "URL=[]   #Taking an empty list\n",
    "\n",
    "#Appending the url of first 10 laptops to empty list\n",
    "for i in laptop_urls[:10]:\n",
    "    URL.append(i.get_attribute('href'))   #Getting url alone\n",
    "URL    \n",
    "\n",
    "#Extracting the ratings of the laptop by using exception as some products dont have any ratings\n",
    "Ratings=[]   #Empty list\n",
    "#Loop for every laptops in the list\n",
    "for url in URL:\n",
    "    driver.get(url)\n",
    "    try:   #Exception handling by using NoSuchElementException\n",
    "        prod_rating=driver.find_element_by_id('acrCustomerReviewText')  #Locating the rating link\n",
    "        prod_rating.click()\n",
    "        rating=driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-base']\") #Locating the rating tags\n",
    "        Ratings.append(rating.text)  #Appending the text from tags to the list\n",
    "    except NoSuchElementException as e:\n",
    "        Ratings.append('No Rating')  #Appending message for products having no ratings\n",
    "        \n",
    "#Checking out the length of the data extracted\n",
    "print(len(prod_title),len(prod_price),len(Ratings))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo IdeaPad Flex 5 11th Gen Intel Core i7 1...</td>\n",
       "      <td>₹86,990</td>\n",
       "      <td>4.2 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSI Creator 15M, Intel i7-10750H, 15.6\" FHD IP...</td>\n",
       "      <td>₹1,09,990</td>\n",
       "      <td>5 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenovo Legion Y540 Intel Core i7 9th Gen 15.6 ...</td>\n",
       "      <td>₹78,990</td>\n",
       "      <td>4.1 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP 14 Thin &amp; Light 14-inch FHD Laptop (11th Ge...</td>\n",
       "      <td>₹76,500</td>\n",
       "      <td>4.6 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dell Alienware m15(R3) 15.6-inch FHD Gaming La...</td>\n",
       "      <td>₹1,98,590</td>\n",
       "      <td>3 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mi Notebook Horizon Edition 14 Intel Core i5-1...</td>\n",
       "      <td>₹49,999</td>\n",
       "      <td>4.4 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...</td>\n",
       "      <td>₹2,69,990</td>\n",
       "      <td>3.3 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lenovo Yoga S740 Intel Core i7 10th Gen 14 inc...</td>\n",
       "      <td>₹1,35,490</td>\n",
       "      <td>3.4 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lenovo Legion 5Pi 10th Gen Intel Core i7 15.6\"...</td>\n",
       "      <td>₹45,990</td>\n",
       "      <td>4.3 out of 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dell XPS 15 9570 8thGeneration Corei7,16GB RAM...</td>\n",
       "      <td>₹96,061</td>\n",
       "      <td>3.9 out of 5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product name      Price        Rating\n",
       "0  Lenovo IdeaPad Flex 5 11th Gen Intel Core i7 1...    ₹86,990  4.2 out of 5\n",
       "1  MSI Creator 15M, Intel i7-10750H, 15.6\" FHD IP...  ₹1,09,990    5 out of 5\n",
       "2  Lenovo Legion Y540 Intel Core i7 9th Gen 15.6 ...    ₹78,990  4.1 out of 5\n",
       "3  HP 14 Thin & Light 14-inch FHD Laptop (11th Ge...    ₹76,500  4.6 out of 5\n",
       "4  Dell Alienware m15(R3) 15.6-inch FHD Gaming La...  ₹1,98,590    3 out of 5\n",
       "5  Mi Notebook Horizon Edition 14 Intel Core i5-1...    ₹49,999  4.4 out of 5\n",
       "6  ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...  ₹2,69,990  3.3 out of 5\n",
       "7  Lenovo Yoga S740 Intel Core i7 10th Gen 14 inc...  ₹1,35,490  3.4 out of 5\n",
       "8  Lenovo Legion 5Pi 10th Gen Intel Core i7 15.6\"...    ₹45,990  4.3 out of 5\n",
       "9  Dell XPS 15 9570 8thGeneration Corei7,16GB RAM...    ₹96,061  3.9 out of 5"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a new dataframe for saving the data\n",
    "amazon=pd.DataFrame({})\n",
    "amazon['Product name']=prod_title\n",
    "amazon['Price']=prod_price\n",
    "amazon['Rating']=Ratings\n",
    "amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the driver\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
